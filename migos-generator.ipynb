{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jMnKTPknoAVp",
    "outputId": "72dcdd26-42be-47c0-d296-776ec1e99582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 332.1MB 65kB/s \n",
      "\u001b[K     |████████████████████████████████| 3.0MB 39.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 27.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 419kB 58.6MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "WXUzwQuYolIP",
    "outputId": "db6a955d-351e-4a84-cb8e-367ab761ae16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-c2433b82-1f6c-4a4b-bcde-f3fbc0a78396\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-c2433b82-1f6c-4a4b-bcde-f3fbc0a78396\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving migos-bad-and-bougie.txt to migos-bad-and-bougie.txt\n"
     ]
    }
   ],
   "source": [
    "# Upload Migos Text File\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V0nXNRRIqYC0",
    "outputId": "d2c977d4-43a7-499a-846e-3645812674c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 901843 characters\n"
     ]
    }
   ],
   "source": [
    "# This RNN uses individual characters to enagage with text - how many characters are there?\n",
    "text = uploaded[\"migos-bad-and-bougie.txt\"].decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "nl5hli55uBv2",
    "outputId": "127687f4-46c4-4037-df5f-07529323d300",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Offset:]\n",
      "You know young rich niggas\n",
      "You know so we never really had no old money\n",
      "We got a whole lot of new money though, hah\n",
      "(If Young Metro don't trust you I'm gon' shoot ya)\n",
      "Hey\n",
      "\n",
      "Raindrop, drop, drop top (drop top)\n",
      "Smokin' on cookie in the hotbox \n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZp-X8q2uTsl"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "vocab = sorted(set(text))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "woFyHp7luXnf",
    "outputId": "1feb5533-23ed-4183-b932-c5e76fafbb43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '\"' :   3,\n",
      "  '#' :   4,\n",
      "  '$' :   5,\n",
      "  '%' :   6,\n",
      "  '&' :   7,\n",
      "  \"'\" :   8,\n",
      "  '(' :   9,\n",
      "  ')' :  10,\n",
      "  '*' :  11,\n",
      "  ',' :  12,\n",
      "  '-' :  13,\n",
      "  '.' :  14,\n",
      "  '/' :  15,\n",
      "  '0' :  16,\n",
      "  '1' :  17,\n",
      "  '2' :  18,\n",
      "  '3' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# View the character mapping\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "-IdAgg2yudqJ",
    "outputId": "a1a2eb45-8931-42fe-e876-03226b50e83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'[Offset:]\\nYou know young ' ---- characters mapped to int ---- > [56 44 64 64 77 63 78 26 57  0 54 73 79  1 69 72 73 81  1 83 73 79 72 65\n",
      "  1]\n"
     ]
    }
   ],
   "source": [
    "# Example of a mapping of Migos lyrics to a coded array\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:25]), text_as_int[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "5KSNfc9Mu_nn",
    "outputId": "16120159-2a33-408d-af4f-06c877b017fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "O\n",
      "f\n",
      "f\n",
      "s\n",
      "e\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(7):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "fykVVFEkvKtX",
    "outputId": "b2b4334c-148f-4174-baf6-bc37232606c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'[Offset:]\\nYou know young rich niggas\\nYou know so we never really had no old money\\nWe got a whole lot '\n",
      "\"of new money though, hah\\n(If Young Metro don't trust you I'm gon' shoot ya)\\nHey\\n\\nRaindrop, drop, drop\"\n",
      "\" top (drop top)\\nSmokin' on cookie in the hotbox (cookie)\\nFuckin' on your bitch she a thot, thot, thot\"\n",
      "\"\\nCookin' up dope in the crockpot (pot)\\nWe came from nothin' to somethin' nigga (hey)\\nI don't trust no\"\n",
      "'body grip the trigger (nobody)\\nCall up the gang, and they come and get you (gang)\\nCry me a river, giv'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRSHyuI-vRsF"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PqVk6X9-vUan",
    "outputId": "f1201502-ef38-44fb-d635-ccbd40509fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '[Offset:]\\nYou know young rich niggas\\nYou know so we never really had no old money\\nWe got a whole lot'\n",
      "Target data: 'Offset:]\\nYou know young rich niggas\\nYou know so we never really had no old money\\nWe got a whole lot '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "SYZRI5MGvjzp",
    "outputId": "15c8706b-1c36-4333-8881-904f6c60b52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 56 ('[')\n",
      "  expected output: 44 ('O')\n",
      "Step    1\n",
      "  input: 44 ('O')\n",
      "  expected output: 64 ('f')\n",
      "Step    2\n",
      "  input: 64 ('f')\n",
      "  expected output: 64 ('f')\n",
      "Step    3\n",
      "  input: 64 ('f')\n",
      "  expected output: 77 ('s')\n",
      "Step    4\n",
      "  input: 77 ('s')\n",
      "  expected output: 63 ('e')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T8Skysemvs3j",
    "outputId": "3ed99c5b-a6dc-401b-8c7d-a79c5c86fc71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kKhn0-z1v76g"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFZUrUdbv-ek"
   },
   "outputs": [],
   "source": [
    "# Neural Network Architecture\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "     tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    #tf.keras.layers.LSTM(rnn_units,\n",
    "     #                   return_sequences=True,\n",
    "      #                  stateful=True,\n",
    "       #                 recurrent_initializer='glorot_uniform'),  \n",
    "    tf.keras.layers.Dense(vocab_size),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "m-xqqCVKwGxV",
    "outputId": "dbf82aec-84fe-406e-e14a-85ae8624b2b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0531 17:27:32.396944 139664719816576 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f061d6fc9e8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0531 17:27:32.409647 139664719816576 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f05cc52ba20>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "KMA7f-hswInw",
    "outputId": "22991867-4151-4260-e35f-8a30a3867835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 103) # (batch_size, sequence_length, vocab_size)\n",
      "(32, 100, 103) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(2):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "QemrON_AwO5X",
    "outputId": "170caa77-6d97-4518-83be-b63d4bee2318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           26368     \n",
      "_________________________________________________________________\n",
      "unified_lstm (UnifiedLSTM)   (32, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM) (32, None, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 103)           105575    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, None, 103)           10712     \n",
      "=================================================================\n",
      "Total params: 13,782,335\n",
      "Trainable params: 13,782,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "LGnZfwulwgot",
    "outputId": "6203700f-4b87-4d5b-92db-cd7aef462895"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([102,  23,   6,  45,  60,  86,  43,   0,  25,  66,  28,   0,  96,\n",
       "        10, 100,  20,  61,  42,  42,  72,  49,   1,  46,  45,  21,  89,\n",
       "        80,  38,  53,  91,  39,  82,   1,  47,  55,  81,  30,  73,  22,\n",
       "        75,  77,  25,  90,  28,   1,  16, 101,   8,  84,   8,  15,  75,\n",
       "        41,  89,  20,  11,   2,  74,  31,   5,  68,  82,  48,  24,  91,\n",
       "        91,  84,  78,  54,  70,  40,  54,   9,  33,  54,  61,  51,  99,\n",
       "        83,  47,  91,  19,  10, 101,  41,  24,  80,  41,  90,  56,  98,\n",
       "        71,  24,  20,  93,  23,  25,  70,  17,  96])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Nu3lLdXO5q3H",
    "outputId": "3369a54f-0c2e-4f52-f6c8-79906b449b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 100, 103)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.6353507\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guMj8DDc5wjb"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32v3u76r5_8r"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "#EPOCHS=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YelAQZdF5-U0"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2567
    },
    "colab_type": "code",
    "id": "M52XuT3c6GDS",
    "outputId": "d7e6b271-2f7f-4f84-d24e-e89c63d988e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "279/279 [==============================] - 29s 104ms/step - loss: 2.6880\n",
      "Epoch 2/75\n",
      "279/279 [==============================] - 29s 105ms/step - loss: 1.7235\n",
      "Epoch 3/75\n",
      "279/279 [==============================] - 30s 109ms/step - loss: 1.4487\n",
      "Epoch 4/75\n",
      "279/279 [==============================] - 31s 112ms/step - loss: 1.2905\n",
      "Epoch 5/75\n",
      "279/279 [==============================] - 30s 107ms/step - loss: 1.1605\n",
      "Epoch 6/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 1.0415\n",
      "Epoch 7/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.9372\n",
      "Epoch 8/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.8431\n",
      "Epoch 9/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.7543\n",
      "Epoch 10/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.6673\n",
      "Epoch 11/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.5782\n",
      "Epoch 12/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.4998\n",
      "Epoch 13/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.4390\n",
      "Epoch 14/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.3877\n",
      "Epoch 15/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.3350\n",
      "Epoch 16/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.2838\n",
      "Epoch 17/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.2414\n",
      "Epoch 18/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.2136\n",
      "Epoch 19/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1953\n",
      "Epoch 20/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1786\n",
      "Epoch 21/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1700\n",
      "Epoch 22/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1616\n",
      "Epoch 23/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1527\n",
      "Epoch 24/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1436\n",
      "Epoch 25/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1390\n",
      "Epoch 26/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1341\n",
      "Epoch 27/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1317\n",
      "Epoch 28/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1309\n",
      "Epoch 29/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1273\n",
      "Epoch 30/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1257\n",
      "Epoch 31/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1240\n",
      "Epoch 32/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1248\n",
      "Epoch 33/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1186\n",
      "Epoch 34/75\n",
      "279/279 [==============================] - 31s 112ms/step - loss: 0.1163\n",
      "Epoch 35/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1175\n",
      "Epoch 36/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1172\n",
      "Epoch 37/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1178\n",
      "Epoch 38/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1149\n",
      "Epoch 39/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1136\n",
      "Epoch 40/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1130\n",
      "Epoch 41/75\n",
      "279/279 [==============================] - 31s 112ms/step - loss: 0.1147\n",
      "Epoch 42/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1112\n",
      "Epoch 43/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1100\n",
      "Epoch 44/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1104\n",
      "Epoch 45/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1139\n",
      "Epoch 46/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1125\n",
      "Epoch 47/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1110\n",
      "Epoch 48/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1083\n",
      "Epoch 49/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1085\n",
      "Epoch 50/75\n",
      "279/279 [==============================] - 31s 112ms/step - loss: 0.1080\n",
      "Epoch 51/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1082\n",
      "Epoch 52/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1095\n",
      "Epoch 53/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1069\n",
      "Epoch 54/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1073\n",
      "Epoch 55/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1102\n",
      "Epoch 56/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1136\n",
      "Epoch 57/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1116\n",
      "Epoch 58/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1051\n",
      "Epoch 59/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1059\n",
      "Epoch 60/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1075\n",
      "Epoch 61/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1102\n",
      "Epoch 62/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1121\n",
      "Epoch 63/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1119\n",
      "Epoch 64/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1115\n",
      "Epoch 65/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1111\n",
      "Epoch 66/75\n",
      "279/279 [==============================] - 31s 110ms/step - loss: 0.1094\n",
      "Epoch 67/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1084\n",
      "Epoch 68/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1089\n",
      "Epoch 69/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1121\n",
      "Epoch 70/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1107\n",
      "Epoch 71/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1103\n",
      "Epoch 72/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1139\n",
      "Epoch 73/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1139\n",
      "Epoch 74/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1128\n",
      "Epoch 75/75\n",
      "279/279 [==============================] - 31s 111ms/step - loss: 0.1133\n"
     ]
    }
   ],
   "source": [
    "# Train Model - Takes about Half an Hour on a GPU\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6TdNkLQQ6e8-",
    "outputId": "b7f2733b-3a3b-42a3-d76e-f06e258aacb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_75'"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "s9Wdl8Lp6re1",
    "outputId": "00d7edc4-25a4-465b-ec4b-2bd9e052bc78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0531 18:07:06.230292 139664719816576 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f059a5e6fd0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "W0531 18:07:06.235155 139664719816576 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f059a5f8ac8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "RBfeym376u-0",
    "outputId": "9b192ad3-6ab2-4af3-d0e0-c172e4e7026c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            26368     \n",
      "_________________________________________________________________\n",
      "unified_lstm_2 (UnifiedLSTM) (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "unified_lstm_3 (UnifiedLSTM) (1, None, 1024)           8392704   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 103)            105575    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 103)            10712     \n",
      "=================================================================\n",
      "Total params: 13,782,335\n",
      "Trainable params: 13,782,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2aCso-464Ls"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 3000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.1\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1292
    },
    "colab_type": "code",
    "id": "1gVx3FRV67cW",
    "outputId": "d152a175-1462-4d74-8807-08931222bc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook] \n",
      "[Verse 3: Offset]\n",
      "You mught me talking Brokanese, you split em by the bando, know the with them bricks from them boys count I come in my trap\n",
      "But it's all good though\n",
      "\n",
      "[Hook]\n",
      "\n",
      "[Verse 2 - Takeoff:]\n",
      "Takeoff!!\n",
      "When I beat the money I poured her (Trap, andn's your ass up right witcha)\n",
      "Get right witcha (I'm a get right witcha)\n",
      "Bad bitches, fuck 'em then dismiss em (bad, woo)\n",
      "I ain't really here to take no pictures (flash)\n",
      "Middle finger up fuck the system (fuck 'em)\n",
      "\n",
      "Yeah, coupe is ride, yeah\n",
      "Pull up on mistoli, invented that your brother\n",
      "You say you on the block skuuuurt)\n",
      "I'm not gonna tell to then you in Range\n",
      "Now when I wake up I lost my brother T\n",
      "Welr man, who who eart\n",
      "Oh, what is the perkinjachip\n",
      "Straight up out the Camble\n",
      "Wrist spinning like a motor\n",
      "IPhout out the seal, we pour out the seal\n",
      "I ran out of lean, I need a refill\n",
      "We pour out the seal, we pour out the seal\n",
      "I ran out of lean, I need a refill\n",
      "We pour out the seal, we pour out the seal\n",
      "I don't wanna feel down Rollie she do you remember my past\n",
      "Only one come got to go crazy in this Mormis to the game then is it man\n",
      "\n",
      "[Quavo:]\n",
      "We the wave, we the wave, typhoon (wave, aye, typhoon)\n",
      "Take her out of the hills to fuck with some goons\n",
      "Now she breaking pounds in my room (pounds)\n",
      "\n",
      "[Offset:]\n",
      "Smoke you...\n",
      "Broke nigga fall back (fall back and I used to it to All this motherfuckin' off\n",
      "Came in the booth firection\n",
      "You wanna ple routes on a minute\n",
      "Got these bitches is plansime, chill we want to with the Glock\n",
      "And I hit her up, get the pack gone (back it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "In the kitchen, wrist twistin' like it's stir fry (whip it)\n",
      "Switch yo in the kitchen, wrappin' the bar, on the wrist, that's in sofa\n",
      "Break em down, we having a forest\n",
      "Came from the gump no Forrest\n",
      "One eye, counting my house with the [?] on the night\n",
      "\n",
      "[Chorus x2]\n",
      "\n",
      "[Intro:]\n",
      "We're not against I'm a Masi\n",
      "I swear to god the block like he play with the water\n",
      "She wanna fuck, I don’t want her\n",
      "Popped a p\n"
     ]
    }
   ],
   "source": [
    "# Example Song!\n",
    "print(generate_text(model, start_string=u\"[Hook] \\n\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text-gen-rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
